{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_number = 223\n",
    "network_structure = [4, 8, 5]\n",
    "\n",
    "num_epochs = 5\n",
    "num_steps = 50\n",
    "batch_size = 100\n",
    "\n",
    "beta = 0.8\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "sampling_strategy = {1: 90083, 2: 7009, 3: 2779, 4: 803, 5: 15}\n",
    "\n",
    "full_path = \"Tanh[2,8,8,4]-batch-10.png\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv(\"kaggle.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[[\"record\", \"type\", \"0_qrs_interval\", \"0_pre-RR\", \"0_post-RR\", \"0_qt_interval\", \"0_st_interval\"]]\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "type\n",
       "N       90083\n",
       "VEB      7009\n",
       "SVEB     2779\n",
       "F         803\n",
       "Q          15\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"type\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"type\"] = df[\"type\"].map({\"N\": 1, \"VEB\": 2, \"SVEB\": 3, \"F\": 4, \"Q\":5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[[\"0_pre-RR\", \"0_qrs_interval\", \"0_qt_interval\", \"0_st_interval\"]]\n",
    "y = df[\"type\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "\n",
    "sm = SMOTE(random_state=0, sampling_strategy=sampling_strategy)\n",
    "X, y = sm.fit_resample(X=X, y=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = (X - X.mean()) / X.std()\n",
    "y = y.map(\n",
    "    {1: [1, 0, 0, 0, 0], 2: [0, 1, 0, 0, 0], 3: [0, 0, 1, 0, 0], 4: [0, 0, 0, 1, 0] , 5: [0, 0, 0, 0, 1]}\n",
    ")\n",
    "y = np.array(y.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting data for training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = torch.from_numpy(X.values).float()\n",
    "y = torch.from_numpy(y).float()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, train_size=0.7, random_state=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arythmia Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snntorch as snn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Initialize layers\n",
    "        self.fc1 = nn.Linear(network_structure[0],network_structure[1])\n",
    "        self.lif1 = snn.Leaky(beta=beta)\n",
    "        self.fc2 = nn.Linear(network_structure[1], network_structure[2])\n",
    "        self.lif2 = snn.Leaky(beta=beta)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden states at t=0\n",
    "\n",
    "        mem1 = self.lif1.init_leaky()\n",
    "        mem2 = self.lif2.init_leaky()\n",
    "\n",
    "        # Record the final layer\n",
    "        spk2_rec = []\n",
    "        mem2_rec = []\n",
    "\n",
    "        # time-loop\n",
    "        for _ in range(num_steps):\n",
    "            cur1 = self.fc1(x)  \n",
    "            spk1, mem1 = self.lif1(cur1, mem1)\n",
    "            cur2 = self.fc2(spk1)\n",
    "            spk2, mem2 = self.lif2(cur2, mem2)\n",
    "\n",
    "            # store in list\n",
    "            spk2_rec.append(spk2)\n",
    "            mem2_rec.append(mem2)\n",
    "\n",
    "        return torch.stack(spk2_rec).float(), torch.stack(\n",
    "            mem2_rec, dim=0\n",
    "        )  \n",
    "\n",
    "\n",
    "# Load the network onto CUDA if available\n",
    "net = Net().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snntorch.functional as SF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = SF.ce_count_loss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=5e-4, betas=(beta,beta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, loss_fn, X_train, y_train, num_epochs, batch_size=5):\n",
    "\n",
    "  # Calculate number of batches\n",
    "  num_samples = len(X_train)\n",
    "  num_batches = num_samples // batch_size\n",
    "\n",
    "  for epoch in range(num_epochs):\n",
    "    total_correct = 0\n",
    "    for i in range(num_batches):\n",
    "      # Get current batch of data\n",
    "      start_idx = i\n",
    "      end_idx = min(i + batch_size, num_samples)\n",
    "      x_batch = X_train[start_idx:end_idx]\n",
    "      y_batch = y_train[start_idx:end_idx]\n",
    "\n",
    "      x_batch = x_batch.to(device)\n",
    "      y_batch_idx = y_batch.max(1)[1]  \n",
    "      y_batch_idx = y_batch_idx.to(device)\n",
    "\n",
    "      # Forward pass\n",
    "      spk2, _ = model(x_batch)\n",
    "      \n",
    "      # Calculate loss\n",
    "      loss = loss_fn(spk2, y_batch_idx)\n",
    "\n",
    "      accuracy = SF.acc.accuracy_rate(spk2, y_batch_idx)\n",
    "      total_correct += accuracy.item() * len(y_batch) \n",
    "\n",
    "      # Backward pass and optimize\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "    average_accuracy = total_correct / num_samples\n",
    "    print(f\"Epoch: {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}, Accuracy: {average_accuracy:.4f}\")\n",
    "\n",
    "def evaluate(model, loss_fn, X_test, y_test, batch_size=5):\n",
    "\n",
    "  # Initialize variables for tracking accuracy and loss\n",
    "  total_correct = 0\n",
    "  num_samples = len(y_test)\n",
    "  total_loss = 0\n",
    "\n",
    "  # Evaluate on test data in batches\n",
    "  with torch.no_grad():\n",
    "    for i in range(0, len(X_test), batch_size):\n",
    "      # Get current batch of data\n",
    "      start_idx = i\n",
    "      end_idx = min(i + batch_size, num_samples)\n",
    "      x_batch = X_test[start_idx:end_idx]\n",
    "      y_batch = y_test[start_idx:end_idx]\n",
    "\n",
    "      x_batch = x_batch.to(device)\n",
    "      y_batch_idx = y_batch.max(1)[1]  \n",
    "      y_batch_idx = y_batch_idx.to(device)\n",
    "\n",
    "      # Forward pass\n",
    "      spk2, _ = model(x_batch)\n",
    "\n",
    "      # Calculate accuracy\n",
    "      accuracy = SF.acc.accuracy_rate(spk2, y_batch_idx)\n",
    "      total_correct += accuracy.item() * len(y_batch)  \n",
    "\n",
    "      # Calculate loss\n",
    "      loss = loss_fn(spk2, y_batch_idx)  \n",
    "      total_loss += loss.item() * len(y_batch) \n",
    "\n",
    "  # Calculate and print average accuracy and loss\n",
    "  average_accuracy = total_correct / num_samples\n",
    "  average_loss = total_loss / num_samples\n",
    "  print(f\"Test Accuracy: {average_accuracy:.4f}\")\n",
    "  print(f\"Test Loss: {average_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/5, Loss: 0.3913, Accuracy: 0.8422\n",
      "Epoch: 2/5, Loss: 0.3058, Accuracy: 0.9296\n",
      "Epoch: 3/5, Loss: 0.3159, Accuracy: 0.9334\n",
      "Epoch: 4/5, Loss: 0.3254, Accuracy: 0.9378\n",
      "Epoch: 5/5, Loss: 0.3546, Accuracy: 0.9393\n"
     ]
    }
   ],
   "source": [
    "train(net, optimizer, loss_fn, X_train, y_train, num_epochs=num_epochs, batch_size=batch_size)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9215\n",
      "Test Loss: 0.3914\n"
     ]
    }
   ],
   "source": [
    "evaluate(net, loss_fn, X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
